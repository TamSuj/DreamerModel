{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bd1df50a",
   "metadata": {},
   "source": [
    "## All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d2abe96-4005-4639-9e41-5f09991c912a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m \n\u001b[1;32m     11\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m accuracy_score, classification_report\n\u001b[1;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m \u001b[39mimport\u001b[39;00m StandardScaler\n\u001b[1;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m train_test_split\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "'''\n",
    "    Pandas: for loading and manipulating the CSV datasets as dataframes\n",
    "    Numpy: for numerical operations on arrays/matrices\n",
    "    Sklearn metrics: for evaluating model performance (accuracy, classification report etc) - sklearn preprocessing: for data standardization\n",
    "    Sklearn model selection: for splitting data into train/test sets\n",
    "    Keras: for building and training neural network models\n",
    "    Keras callbacks: for early stopping to prevent overfitting\n",
    "'''\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Step 2: Load and Prepare Data\n",
    "data_1 = pd.read_csv(r'C:\\Users\\bravo\\OneDrive\\OneDrive Files\\Desktop\\train_set_1.csv')\n",
    "data_2 = pd.read_csv(r'C:\\Users\\bravo\\OneDrive\\OneDrive Files\\Desktop\\train_set_2.csv')\n",
    "data_3 = pd.read_csv(r'C:\\Users\\bravo\\OneDrive\\OneDrive Files\\Desktop\\train_set_3.csv')\n",
    "\n",
    "# Step 3: Generate Features for Financial Time Series Data\n",
    "def generate_features(data):\n",
    "    lag = 5\n",
    "    data['SMA_5'] = data['value'].rolling(window=5).mean()\n",
    "    data['SMA_20'] = data['value'].rolling(window=20).mean()\n",
    "\n",
    "    for i in range(1, lag + 1):\n",
    "        data[f'Lag_{i}'] = data['value'].shift(i)\n",
    "    \n",
    "    data['Rolling_STD_5'] = data['value'].rolling(window=5).std()\n",
    "    data['Rolling_STD_20'] = data['value'].rolling(window=20).std()\n",
    "    \n",
    "    roc_period = 5\n",
    "    data['ROC'] = (data['value'].diff(roc_period).shift(-1) > 0).astype(int)  # Shift ROC as required\n",
    "\n",
    "    return data\n",
    "\n",
    "data_1 = generate_features(data_1)\n",
    "data_2 = generate_features(data_2)\n",
    "data_3 = generate_features(data_3)\n",
    "\n",
    "# Step 4: Prepare Features and Labels for all Datasets\n",
    "def prepare_data(data):\n",
    "    lag = 5\n",
    "    data = data.dropna()\n",
    "    \n",
    "    X = data[['SMA_5', 'SMA_20', 'Rolling_STD_5', 'Rolling_STD_20'] + [f'Lag_{i}' for i in range(1, lag + 1)]]\n",
    "    y = data['ROC']\n",
    "\n",
    "    return X, y\n",
    "\n",
    "X_1, y_1 = prepare_data(data_1)\n",
    "X_2, y_2 = prepare_data(data_2)\n",
    "X_3, y_3 = prepare_data(data_3)\n",
    "\n",
    "# Step 5: Split Data into Training and Test Sets for all Datasets\n",
    "def split_data(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Step 6: Train and Evaluate Models for all Datasets\n",
    "def train_and_evaluate_model(X_train, y_train, X_test, y_test, model):\n",
    "    model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2, callbacks=[early_stopping], verbose=0)\n",
    "    y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"Test Accuracy:\", accuracy)\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"Evaluation for Dataset 1:\")\n",
    "X_train_1, X_test_1, y_train_1, y_test_1 = split_data(X_1, y_1)\n",
    "model_1 = Sequential()\n",
    "model_1.add(Dense(32, input_dim=X_train_1.shape[1], activation='relu'))\n",
    "model_1.add(Dense(1, activation='sigmoid'))\n",
    "model_1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "early_stopping = EarlyStopping(patience=10, restore_best_weights=True)\n",
    "train_and_evaluate_model(X_train_1, y_train_1, X_test_1, y_test_1, model_1)\n",
    "\n",
    "print(\"Evaluation for Dataset 2:\")\n",
    "X_train_2, X_test_2, y_train_2, y_test_2 = split_data(X_2, y_2)\n",
    "model_2 = Sequential()\n",
    "model_2.add(Dense(32, input_dim=X_train_2.shape[1], activation='relu'))\n",
    "model_2.add(Dense(1, activation='sigmoid'))\n",
    "model_2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "early_stopping = EarlyStopping(patience=10, restore_best_weights=True)\n",
    "train_and_evaluate_model(X_train_2, y_train_2, X_test_2, y_test_2, model_2)\n",
    "\n",
    "print(\"Evaluation for Dataset 3:\")\n",
    "X_train_3, X_test_3, y_train_3, y_test_3 = split_data(X_3, y_3)\n",
    "model_3 = Sequential()\n",
    "model_3.add(Dense(32, input_dim=X_train_3.shape[1], activation='relu'))\n",
    "model_3.add(Dense(1, activation='sigmoid'))\n",
    "model_3.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "early_stopping = EarlyStopping(patience=10, restore_best_weights=True)\n",
    "train_and_evaluate_model(X_train_3, y_train_3, X_test_3, y_test_3, model_3)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c1d5472b",
   "metadata": {},
   "source": [
    "## Background/Key Points ðŸ“ƒ"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4e8e78b5",
   "metadata": {},
   "source": [
    "### Simple moving average (SMA):\n",
    "- Takes the average of the time series over a specified window\n",
    "- Helps smooth out noise and reveal underlying trends\n",
    "- E.g. 5-period SMA shows short term trend, 20-period shows longer term\n",
    "\n",
    "### Lagging features:\n",
    "- Shifting the time series values backwards in time - Provides historical context for each point\n",
    "- E.g. Lag_1 is the value from 1 time period ago\n",
    "\n",
    "### Rolling standard deviation:\n",
    "- Measures volatility by taking standard deviation over a window - Higher values indicate more volatility/fluctuation\n",
    "\n",
    "### Rate of change (ROC):\n",
    "- Binary indicator showing if value increased or decreased - Compares current value to value 'n' periods ago\n",
    "- Used as label for binary classification models\n",
    "\n",
    "This is a binary classification model because the target variable, ROC, only takes on two values (0 or 1). The goal is to predict if the time series is going up or down.\n",
    "\n",
    "Binary classification models predict one of two outcomes, like yes/no, true/false, spam/not-spam etc.\n",
    "\n",
    "### Some common binary classification algorithms are: \n",
    "- Logistic regression \n",
    "- SVM (Support Vector Machine) - Neural networks\n",
    "- Naive Bayes\n",
    "- Decision trees\n",
    "\n",
    "The code uses a neural network for binary classification because they can capture complex relationships between the input features like SMA, volatility etc. and the uptrend/downtrend prediction. The sigmoid output activation squashes predictions between 0 and 1 for probability-like outputs.\n",
    "\n",
    "In summary, the engineered features help uncover patterns that are fed into a neural network binary classifier to predict if the financial time series is going up or down.\n",
    "\n",
    "### Neural Networks:\n",
    "A neural network is a type of machine learning model loosely inspired by biological neurons in the brain. The goal is to approximate complex mathematical functions that map input features to output values.\n",
    "Some key aspects of how they work:\n",
    "- Made up of layers of \"neurons\" or nodes\n",
    "- Input layer receives the feature data\n",
    "- Hidden layers apply transformations and enable learning of complex patterns - Output layer makes predictions\n",
    "- Nodes are densely connected across layers\n",
    "- Connections have weights that are learned during training\n",
    "- Data flows through the network in a forward pass - Predictions are made based on current weights\n",
    "- Loss is calculated against true labels\n",
    "- Weights are adjusted through backpropagation to reduce loss\n",
    "- Learning involves finding the right weight values that minimize loss In this code, a simple neural net with 1 hidden layer is defined:\n",
    "- Input layer size determined by the number of time series features\n",
    "- 1 hidden layer with 32 nodes to learn complex patterns\n",
    "- Output layer is a single sigmoid node for binary classification\n",
    "During training:\n",
    "- Data flows through, predictions made\n",
    "- Loss calculated against true uptrend/downtrend labels - Backprop adjusts weights to reduce loss\n",
    "- Process repeats for multiple epochs\n",
    "\n",
    "After training, the learned model can predict on new data. The neural network essentially learns complex mathematical functions that map the input time series features to the likelihood of an increasing or decreasing trend.\n",
    "\n",
    "In summary, neural networks learn to unpack complex relationships between the inputs and outputs based on backpropagating loss, making them very versatile and powerful models.\n",
    "\n",
    "#### Neural Network Implementation:\n",
    "- Keras is used to define and train the neural network model in the code.\n",
    "- A Sequential model defines a linear stack of layers.\n",
    "- The first layer is the Dense input layer. It has 32 nodes and a ReLU activation function.\n",
    "- The input dimension is set to the number of features in X_train, so it matches the feature data.\n",
    "- The second layer is a single output node with a sigmoid activation. This squashes outputs between 0 and 1.\n",
    "- The loss function used is binary cross-entropy, appropriate for binary classification. - The adam optimizer helps adjust weights during backpropagation.\n",
    "- Metrics like accuracy can be monitored during training.\n",
    "\n",
    "#### Neural Network Training:\n",
    "   \n",
    "- The network is trained for 50 epochs with a batch size of 32. - A validation split of 0.2 is used to monitor overfitting.\n",
    "- Early stopping callbacks prevent overfitting by stopping if validation loss doesn't improve after 10 epochs.\n",
    "- Verbose=0 suppresses logging output for cleaner results.\n",
    "- The fit() method trains the model by propagating data through, calculating loss with respect to labels, and updating weights via backpropagation.\n",
    "\n",
    "#### Making Predictions:\n",
    "- After training, the model can make predictions on new data.\n",
    "- The predict() method runs a forward pass, returning the probability outputs. - A threshold of 0.5 is applied to make binary predictions.\n",
    "\n",
    "So in summary, Keras provides a high-level API to define, train and use neural network models seamlessly in Python. The code leverages this to effectively build and apply a neural network for time series classification.\n",
    "\n",
    "### Long short-term memory (LSTM):\n",
    "- LSTM is a type of neural network model that is well-suited for processing sequential data like\n",
    "time series or text data.\n",
    "- It is a special kind of recurrent neural network (RNN). RNNs have looping connections that\n",
    "allow information to persist across time steps.\n",
    "- The problem with basic RNNs is that they can struggle with long-term dependencies in\n",
    "sequences. This is where LSTMs help.\n",
    "- LSTMs have a more complex structure than basic RNNs, with special units called memory\n",
    "cells.\n",
    "- These memory cells can store information for long periods of time. They have gates that\n",
    "control when to store, use, or forget information.\n",
    "- This gives LSTMs the ability to learn long-term dependencies in sequence data that basic\n",
    "RNNs struggle with.\n",
    "- So in summary, LSTM is a type of RNN that uses special memory cells to store and access\n",
    "information over long sequences, allowing it to model temporal data effectively.\n",
    "\n",
    "### RNN\n",
    "- An RNN is a type of neural network designed to process sequential data, like time series or\n",
    "text.\n",
    "- It has a recurrent layer that maintains a state over time.\n",
    "- This state captures information about previous inputs seen by the network.\n",
    "- So the RNN \"remembers\" prior inputs as it processes the next input in a sequence.\n",
    "- This memory of previous context is what gives RNNs their unique capabilities for sequence\n",
    "modeling.\n",
    "For example, in a time series forecasting task:\n",
    "- The RNN takes in a sequence of data points as input one at a time.\n",
    "- As it sees each input, the state of the recurrent layer gets updated based on current input and\n",
    "previous context.\n",
    "- So the network retains memory of previous time steps' context.\n",
    "- This contextual information gets passed forward and helps the RNN predict the next value in\n",
    "the sequence.\n",
    "- Without the recurrent connection, regular neural networks have no memory and treat each input\n",
    "independently.\n",
    " \n",
    "In summary,\n",
    "- RNNs have feedback loops that give them memory over a sequence.\n",
    "- This lets them learn temporal dynamic behavior, unlike regular neural networks.\n",
    "- The recurrent layer state captures relevant context from prior inputs to inform predictions.\n",
    "So RNN models are ideal for processing sequence data like time series, text, audio, etc. Their\n",
    "memory over sequences makes them uniquely suited for such tasks."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "34f6889d",
   "metadata": {},
   "source": [
    "## Data Preperation âš™ï¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2d2dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "'''\n",
    "    Pandas: for loading and manipulating the CSV datasets as dataframes\n",
    "    Numpy: for numerical operations on arrays/matrices\n",
    "    Sklearn metrics: for evaluating model performance (accuracy, classification report etc) - sklearn preprocessing: for data standardization\n",
    "    Sklearn model selection: for splitting data into train/test sets\n",
    "    Keras: for building and training neural network models\n",
    "    Keras callbacks: for early stopping to prevent overfitting\n",
    "'''\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da713949-375f-438a-8d31-99e583eb6d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Load and Prepare Data\n",
    "\n",
    "'''\n",
    "Read each CSV file from the given file path into a separate pandas dataframe \n",
    "Making raw time series data is now accessible for preprocessing\n",
    "'''\n",
    "FILE_PATH = \"C:\\Users\\bravo\\OneDrive\\OneDrive Files\\Desktop\\\\\"\n",
    "\n",
    "data_1 = pd.read_csv(r'FILE_PATH + train_set_1.csv')\n",
    "data_2 = pd.read_csv(r'FILE_PATH + train_set_2.csv')\n",
    "data_3 = pd.read_csv(r'FILE_PATH + train_set_3.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e5a683f1",
   "metadata": {},
   "source": [
    "## Methods (function) ðŸ”‘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f877d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "- Compute simple moving averages (SMA) over 5 and 20 time periods to capture short and long term trends\n",
    "- Compute lagged values of the time series, shifting values backwards in time. This captures historical context.\n",
    "- Compute rolling standard deviation to measure volatility\n",
    "- Generate 'Rate of change' binary indicator showing if value increased or decreased over a period\n",
    "'''\n",
    "def generate_features(data):\n",
    "    lag = 5\n",
    "    data['SMA_5'] = data['value'].rolling(window=5).mean()\n",
    "    data['SMA_20'] = data['value'].rolling(window=20).mean()\n",
    "\n",
    "    for i in range(1, lag + 1):\n",
    "        data[f'Lag_{i}'] = data['value'].shift(i)\n",
    "    \n",
    "    data['Rolling_STD_5'] = data['value'].rolling(window=5).std()\n",
    "    data['Rolling_STD_20'] = data['value'].rolling(window=20).std()\n",
    "    \n",
    "    roc_period = 5\n",
    "    data['ROC'] = (data['value'].diff(roc_period).shift(-1) > 0).astype(int)  # Shift ROC as required\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c400d953",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "- X contains all the engineered features\n",
    "- y contains the binary 'rate of change' labels - Split features and labels for modeling\n",
    "'''\n",
    "def prepare_data(data):\n",
    "    lag = 5\n",
    "    data = data.dropna()\n",
    "    \n",
    "    X = data[['SMA_5', 'SMA_20', 'Rolling_STD_5', 'Rolling_STD_20'] + [f'Lag_{i}' for i in range(1, lag + 1)]]\n",
    "    y = data['ROC']\n",
    "\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a139a7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Split Data into Training and Test Sets for all Datasets\n",
    "'''\n",
    "- Split each dataset into 80% train and 20% test sets\n",
    "- Train set is used to fit models, test set is used for evaluation\n",
    "'''\n",
    "def split_data(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7fc37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Train and Evaluate Models for all Datasets\n",
    "'''\n",
    "Fit the model using the training set, evaluate on test set. Calculate accuracy, classification report etc\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "X_train : feature matrix of the training dataset\n",
    "y_train : target or label values for the training dataset\n",
    "X_test : feature matrix of the testing dataset\n",
    "y_test : target values for the testing dataset\n",
    "model : machine learning model to be trained\n",
    "\n",
    "Returns\n",
    "-------\n",
    "None\n",
    "\n",
    "stdout\n",
    "-------\n",
    "Ex:\n",
    "Test Accuracy: 1.0\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       1.00      1.00      1.00      2017\n",
    "           1       1.00      1.00      1.00      1980\n",
    "\n",
    "    accuracy                           1.00      3997\n",
    "   macro avg       1.00      1.00      1.00      3997\n",
    "weighted avg       1.00      1.00      1.00      3997\n",
    "        \n",
    "'''\n",
    "def train_and_evaluate_model(X_train, y_train, X_test, y_test, model):\n",
    "    model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2, callbacks=[early_stopping], verbose=0)\n",
    "    y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"Test Accuracy:\", accuracy)\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4785b21d",
   "metadata": {},
   "source": [
    "## Implementation ðŸ’»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cc3239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Generate Features for Financial Time Series Data\n",
    "data_1 = generate_features(data_1)\n",
    "data_2 = generate_features(data_2)\n",
    "data_3 = generate_features(data_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f3742e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Prepare Features and Labels for all Datasets\n",
    "X_1, y_1 = prepare_data(data_1)\n",
    "X_2, y_2 = prepare_data(data_2)\n",
    "X_3, y_3 = prepare_data(data_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbd51cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluation for Dataset 1:\")\n",
    "X_train_1, X_test_1, y_train_1, y_test_1 = split_data(X_1, y_1)\n",
    "model_1 = Sequential()\n",
    "model_1.add(Dense(32, input_dim=X_train_1.shape[1], activation='relu'))\n",
    "model_1.add(Dense(1, activation='sigmoid'))\n",
    "model_1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "early_stopping = EarlyStopping(patience=10, restore_best_weights=True)\n",
    "train_and_evaluate_model(X_train_1, y_train_1, X_test_1, y_test_1, model_1)\n",
    "\n",
    "print(\"Evaluation for Dataset 2:\")\n",
    "X_train_2, X_test_2, y_train_2, y_test_2 = split_data(X_2, y_2)\n",
    "model_2 = Sequential()\n",
    "model_2.add(Dense(32, input_dim=X_train_2.shape[1], activation='relu'))\n",
    "model_2.add(Dense(1, activation='sigmoid'))\n",
    "model_2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "early_stopping = EarlyStopping(patience=10, restore_best_weights=True)\n",
    "train_and_evaluate_model(X_train_2, y_train_2, X_test_2, y_test_2, model_2)\n",
    "\n",
    "print(\"Evaluation for Dataset 3:\")\n",
    "X_train_3, X_test_3, y_train_3, y_test_3 = split_data(X_3, y_3)\n",
    "model_3 = Sequential()\n",
    "model_3.add(Dense(32, input_dim=X_train_3.shape[1], activation='relu'))\n",
    "model_3.add(Dense(1, activation='sigmoid'))\n",
    "model_3.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "early_stopping = EarlyStopping(patience=10, restore_best_weights=True)\n",
    "train_and_evaluate_model(X_train_3, y_train_3, X_test_3, y_test_3, model_3)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4 (v3.11.4:d2340ef257, Jun  6 2023, 19:15:51) [Clang 13.0.0 (clang-1300.0.29.30)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
